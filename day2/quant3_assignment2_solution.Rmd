---
title: "Exercise 2 - Linear Regression as a predictive method"
author: "Ken Benoit"
output: html_document
---

For this exercise, you should work with the file `dail2002.dta` from the article Kenneth Benoit and Michael Marsh. 2008. "[The Campaign Value of Incumbency: A New Solution to the Puzzle of Less Effective Incumbent Spending.](http://www.kenbenoit.net/pdfs/ajps_348.pdf)" *American Journal of Political Science* 52(4, October): 874-890.   
    
Load the Stata dataset used in this paper, available from http://www.kenbenoit.net/files/dail2002.dta.  To load this into R, you will need the `read.dta` command from the `foreign` package.  (Note that you can load straight from the URL using this command.)  Call this data object `dail2002`.
    
1.  **Partitioning the dataset into "folds".**  For this exercise, we will be fitting a model from 
    a subset of the data, and using the fitted model to predict the outcomes from the "left out" set 
    and using that to evaluate RMSE or accuracy.
    
    1. To start, familiarize yourself with the `sample()` command to draw a random sample of one-fifth of the observations in the `dail2002.dta` dataset.  Use a `.Random.seed` and read about this function to see what it does.  Why should you use this?
    
    2. For the categorical predictions, we will also assess predictive ability based on "leave-one-out" testing and using "folds", which are groups of observations that have been left out of the training set, and then attempting to predict them with accuracy.  You will need to use indexing to partition the dataset to carry this out.  You might want to write a loop for this.  For instance, to partition a group of 20 observations into four sets, you could use the following `for` loop:
    
    ```{r}
    n <- 20
    k <- 4
    data <- data.frame(myIndex = 1:n, letter = LETTERS[1:n])
    size <- n/4
    if (n %% 4)
        stop("n not divisible by k")
    for (i in 1:k) {
        startIndex <- 1 + (i-1)*size
        endIndex <- startIndex + size - 1
        cat(startIndex, endIndex, "\n")
        print(data[startIndex : endIndex, ])
    }
    ```

    What is the purpose of the line `if (n %% 4)`?
    
    **The `%%` is a modulo operator, which yields the remainder following division of the first argument by the second.  The operation here ensures that the sample size is divisible by `k`, since the `if` statement will only evaluate to true if `(n %% k)` evaluates to a non-zero value.**


2.  **OLS regression for prediction.**

    1.  Fit a regression from the dataset to predict `votes1st`.  You may use any combination of
        regressors that you wish.  Save the model object to `reg2_1`.
        
        ```{r}
        require(foreign)
        dail2002 <- read.dta("http://www.kenbenoit.net/files/dail2002.dta")
        reg2_1 <- lm(votes1st ~ incumb:spend_total + senator + gender + electorate, data=dail2002)
        ```
        
    2.  Predict the `votes1st` from the same sample to which you fitted the regression.  What is the 
        Root Mean Squared Error (RMSE) and how would you interpret this?
        ```{r}
        length(predict(reg2_1))   # shortened to 462 because of the two missing cases
        # refit model with different behaviour for na.action
        reg2_1 <- lm(votes1st ~ incumb*spend_total + senator + gender + electorate, 
                     data=dail2002, na.action=na.exclude)
        reg2_1pred <- predict(reg2_1)
        length(reg2_1pred)  # now includes missing cases
        sum(is.na(reg2_1pred))

        # function to compute RMSE
        computeRMSE <- function(lmobject) {
            sqrt(sum(residuals(lmobject)^2, na.rm=TRUE) / df.residual(lmobject))
        }
        computeRMSE(reg2_1)
        
        # compare to output from summary()
        summary(reg2_1)
        ```

    3.  Drop the incumbency variable -- that you hopefully included in your answer to 2.1! -- and
        repeat steps 2.1--2.2.  Compute a new RMSE and compare this to the previous one.  Which
        is a better predictor?
        ```{r}
        reg2_3 <- lm(votes1st ~ spend_total + senator + gender + electorate, 
                     data=dail2002, na.action=na.exclude)
        computeRMSE(reg2_3)
        summary(reg2_3)$r.squared
        ```
        **The model with incumbency is a better predictor, with a lower RMSE and a higher $R^2$.**
         
3.  **Logistic regression for prediction**.

    1.  Fit a logistic regression (hint: use `glm()`) to predict the outcome variable `wonseat`.  Use 
        any specification that you think provides a good prediction.
        
        ```{r}
        reg3_1 <- glm(wonseat ~ incumb:spend_total + senator + gender + electorate, data=dail2002, family=binomial, na.action=na.exclude)
        ```
    
    2.  For the full sample, compute:
        
        *  a table of actual `wonseat` by predicted `wonseat`
        ```{r}
        wonseat_predicted <- ifelse(predict(reg3_1) > 0, 1, 0)
        (predtable <- table(wonseat_predicted, dail2002$wonseat))
        ```
        
        *  percent correctly predicted
        ```{r}
        sum(diag(prop.table(predtable))) * 100
        ```

        *  precision
        *  recall
        ```{r}
        # define a general function to compute precision & recall
        precrecall <- function(mytable, verbose=TRUE) {
            truePositives <- mytable[1,1]
            falsePositives <- sum(mytable[1,]) - truePositives
            falseNegatives <- sum(mytable[,1]) - truePositives
            precision <- truePositives / (truePositives + falsePositives)
            recall <- truePositives / (truePositives + falseNegatives)
            if (verbose) {
                print(mytable)
                cat("\n precision =", round(precision, 2), 
                    "\n    recall =", round(recall, 2), "\n")
            }
            invisible(c(precision, recall))
        }
        # compute precision of wonseat (not lost seat!)
        precrecall(predtable[2:1, 2:1])
        ```


    3.  Comparing two models.
    
        *  Compute an 8-fold validation, where for 8 different training sets consisting of 7/8 of the observations, you predict the other held-out 1/8 and compare the actual to predicted for the 1/8 test set.  Compute an average F1 score for the 8 models.
        
        ```{r}
        # here it helps to know that n=464 and 464 %% 8 == 0
        foldIndex <- rep(1:8, each = nrow(dail2002)/8)
        # initialize vector to store F1 scores
        f1result <- c()
        # loop over folds
        cat("fold number: ")
        for (i in 1:max(foldIndex)) {
            cat(i)
            glmresult <- glm(wonseat ~ incumb:spend_total + senator + gender + electorate, 
                             na.action = na.exclude, family = "binomial",
                             data = subset(dail2002, foldIndex != i))
            wonseatPred <- ifelse(predict(glmresult, newdata = subset(dail2002, foldIndex == i)) > 0, 1, 0)
            predtable <- table(wonseatPred, subset(dail2002, foldIndex == i)$wonseat)
            prrec <- precrecall(predtable[2:1, 2:1], verbose=FALSE)
            f1result[i] <- 2 * prod(prrec) / sum(prrec)
        }
        cat("\n")
        f1result
        mean(f1result)
        ```
        
        *  Now drop a variable or two, and repeat the previous step to compare the average F1 score for this model.
        ```{r}
        f1result <- c()
        # loop over folds
        for (i in 1:8) {
            glmresult <- glm(wonseat ~ spend_total + senator + gender + electorate, 
                             na.action = na.exclude, family = "binomial",
                             data = subset(dail2002, foldIndex != i))
            wonseatPred <- ifelse(predict(glmresult, newdata = subset(dail2002, foldIndex == i)) > 0, 1, 0)
            predtable <- table(wonseatPred, subset(dail2002, foldIndex == i)$wonseat)
            prrec <- precrecall(predtable[2:1, 2:1], verbose=FALSE)
            f1result[i] <- 2 * prod(prrec) / sum(prrec)
        }
        f1result
        mean(f1result)
        ```
        *  Why is it valuable to use the different folds here, rather than simply comparing the F1 score for the predicted outcome of the entire sample, fit to the entire sample?
        **Because fitting on the whole sample can lead to overfitting.  For calibrating a predictive model we need to test it out of sample.**
    
      
  
